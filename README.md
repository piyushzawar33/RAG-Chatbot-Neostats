# RAG-Chatbot-Neostats
A Gemini-powered end to end RAG pipeline with FAISS and optional web search fallback to answer questions based on podcast transcripts.


# Podcast Q&A Assistant

This project is an end-to-end Retrieval-Augmented Generation (RAG) system. It answers questions related to the Elon Musk and Nikhil Kamath's podcast using a combination of local transcript retrieval and Gemini LLM reasoning, with optional web search fallback.

The system uses:
- FAISS for vector retrieval
- Gemini embeddings and Gemini LLM for generation
- Tavily for optional external web search
- Streamlit for the user interface

---

## Features

- Transcript-based retrieval using FAISS
- Gemini text embeddings for vector indexing
- Multi-chunk retrieval (Top-3) for improved context quality
- Optional Tavily web search fallback when transcript information is insufficient
- Support for concise and detailed answer styles
- Streamlit interface with expandable debug information

---

## How It Works

1. The podcast transcript is preprocessed and split into overlapping chunks.
2. Each chunk is embedded using Gemini text-embedding-004.
3. A FAISS index is built from these embeddings.
4. When a user asks a question:
   - The query is embedded
   - The Top-3 most relevant chunks are retrieved from the FAISS index
   - If similarity is low, a web search fallback is triggered
5. The final answer is generated by the Gemini LLM using the retrieved context (and web context if applicable).

---


---

## Setup Instructions

### 1. Create a virtual environment
python -m venv .venv


### 2. Activate it
.venv\Scripts\activate


### 3. Install dependencies
pip install -r requirements.txt


### 4. Create a `.env` file in the project root
GEMINI_API_KEY=your_api_key
GEMINI_CHAT_MODEL=models/gemini-2.5-flash
GEMINI_EMBED_MODEL=models/text-embedding-004
TAVILY_API_KEY=your_tavily_api_key


### 5. Add the transcript
Place your manually cleaned transcript at:
data/cleaned_transcript.txt


### 6. Run the Streamlit application
streamlit run app.py


---

## Usage

- Enter a question related to the podcast.
- Choose concise or detailed answer mode.
- Enable web search fallback if required.
- View transcript-based retrieval results and optional web search output in the debug panels.

---

## Summary

This project demonstrates a clean and efficient implementation of a hybrid RAG pipeline using Gemini, FAISS, and Tavily. It is designed to be simple to understand, easy to extend, and suitable for evaluation in the NeoStats AI Engineer selection process.


